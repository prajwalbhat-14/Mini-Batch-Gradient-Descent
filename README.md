# Mini-Batch-Gradient-Descent
Mini-Batch Gradient Descent updates model parameters using small random subsets (mini-batches) of the training data. It offers a balance between speed and accuracy, reducing noise compared to Stochastic Gradient Descent and being faster than Batch Gradient Descent, which makes it more efficient for large datasets.
